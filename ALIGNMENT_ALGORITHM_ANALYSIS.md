# Alignment AlgoritmasÄ± DetaylÄ± Analiz Raporu

## ğŸ“‹ Ã–zet

Bu rapor, Okuma Analizi sistemindeki **Alignment AlgoritmasÄ±** (`backend/app/services/alignment.py`) Ã¼zerinde yapÄ±lan detaylÄ± teknik analizi iÃ§ermektedir. Algoritma, TÃ¼rkÃ§e metinlerde konuÅŸma tanÄ±ma sonuÃ§larÄ± ile hedef metinler arasÄ±nda kelime seviyesinde hizalama yapmak ve okuma hatalarÄ±nÄ± tespit etmek iÃ§in kullanÄ±lmaktadÄ±r.

## ğŸ¯ Algoritma AmacÄ±

Alignment algoritmasÄ± ÅŸu temel gÃ¶revleri yerine getirir:

1. **Reference Text** (hedef metin) ile **Hypothesis Text** (konuÅŸma tanÄ±ma sonucu) arasÄ±nda kelime seviyesinde hizalama
2. **Okuma hatalarÄ±nÄ±n tespiti**: eksik, fazla, deÄŸiÅŸtirilmiÅŸ, tekrarlanan kelimeler
3. **TÃ¼rkÃ§e dil Ã¶zelliklerine uygun** hata sÄ±nÄ±flandÄ±rmasÄ±
4. **Timing bilgilerinin** korunmasÄ± ve event'lerin oluÅŸturulmasÄ±

## ğŸ—ï¸ Algoritma Mimarisi

### 1. Ana Fonksiyonlar

```python
# Ana alignment fonksiyonu
def levenshtein_align(ref_tokens: List[str], hyp_tokens: List[str], 
                     word_times: List[Dict[str, Any]] = None) -> List[Tuple[str, str, str, int, int]]

# Event oluÅŸturma fonksiyonu  
def build_word_events(alignment: List[Tuple[str, str, str, int, int]], word_times: List[Dict[str, Any]]) -> List[Dict[str, Any]]

# TÃ¼rkÃ§e tokenization
def tokenize_tr(text: str) -> List[str]
```

### 2. YardÄ±mcÄ± Fonksiyonlar

```python
# Token normalizasyonu
def _norm_token(tok: str) -> str

# Operation cost hesaplama
def _get_operation_cost(ref_token: str, hyp_token: str, operation: str, 
                       repeated_fillers: Dict[int, bool] = None, hyp_idx: int = -1) -> float

# Filler word tracking
def _track_filler_repetitions(hyp_tokens: List[str], word_times: List[Dict[str, Any]]) -> Dict[int, bool]

# Post-repair logic
def _post_repair_filler_substitutions(alignment: List[Tuple[str, str, str, int, int]]) -> List[Tuple[str, str, str, int, int]]
```

## ğŸ”§ Algoritma BileÅŸenleri DetaylÄ± Analizi

### 1. **Token Normalizasyonu** (`_norm_token`)

#### AmaÃ§
TÃ¼rkÃ§e karakterleri normalize ederek karÅŸÄ±laÅŸtÄ±rma yapmak.

#### Ä°ÅŸleyiÅŸ
```python
def _norm_token(tok: str) -> str:
    # 1. KÃ¼Ã§Ã¼k harfe Ã§evir
    t = tok.lower()
    
    # 2. TÃ¼rkÃ§e karakter normalizasyonu
    t = t.replace('Ä±', 'i')
    t = t.replace('ÄŸ', 'g') 
    t = t.replace('Ã§', 'c')
    t = t.replace('Ã¶', 'o')
    t = t.replace('Ã¼', 'u')
    t = t.replace('ÅŸ', 's')
    
    # 3. Unicode normalizasyonu
    t = unicodedata.normalize('NFD', t)
    t = ''.join(c for c in t if unicodedata.category(c) != 'Mn')
    t = unicodedata.normalize('NFC', t)
    
    # 4. Noktalama temizleme
    t = re.sub(r'[.,!?;:"""]+$', '', t)
    t = re.sub(r'^[.,!?;:"""]+', '', t)
    
    return t
```

#### ğŸš¨ **Kritik Hata NoktasÄ±**
- **Problem**: Normalizasyon Ã§ok agresif, anlam kaybÄ±na neden olabilir
- **Ã–rnek**: `Ä±ÅŸÄ±k` â†’ `isik`, `gÃ¶z` â†’ `goz` 
- **SonuÃ§**: FarklÄ± anlamlÄ± kelimeler aynÄ± olarak algÄ±lanabilir

### 2. **Dynamic Programming Tablosu**

#### AmaÃ§
Levenshtein distance algoritmasÄ± ile optimal alignment bulma.

#### Ä°ÅŸleyiÅŸ
```python
# DP tablosu oluÅŸturma
dp = [[0.0] * (n + 1) for _ in range(m + 1)]

# Base case'ler
for i in range(m + 1):
    dp[i][0] = dp[i-1][0] + _get_operation_cost(ref_tokens[i-1], "", "delete", repeated_fillers, -1)

for j in range(n + 1):
    dp[0][j] = dp[0][j-1] + _get_operation_cost("", hyp_tokens[j-1], "insert", repeated_fillers, j-1)

# DP tablosu doldurma
for i in range(1, m + 1):
    for j in range(1, n + 1):
        ref_token = ref_tokens[i-1]
        hyp_token = hyp_tokens[j-1]
        
        # Normalized equality check
        if _norm_token(ref_token) == _norm_token(hyp_token):
            dp[i][j] = dp[i-1][j-1]
        else:
            # Cost hesaplama
            del_cost = dp[i-1][j] + _get_operation_cost(ref_token, "", "delete", repeated_fillers, -1)
            ins_cost = dp[i][j-1] + _get_operation_cost("", hyp_token, "insert", repeated_fillers, j-1)
            rep_cost = dp[i-1][j-1] + _get_operation_cost(ref_token, hyp_token, "replace", repeated_fillers, j-1)
            
            dp[i][j] = min(del_cost, ins_cost, rep_cost)
```

#### ğŸš¨ **Kritik Hata NoktasÄ±**
- **Problem**: Normalized equality check Ã§ok erken yapÄ±lÄ±yor
- **SonuÃ§**: Context kaybÄ±, yanlÄ±ÅŸ eÅŸleÅŸmeler

### 3. **Operation Cost Hesaplama** (`_get_operation_cost`)

#### AmaÃ§
FarklÄ± operasyonlarÄ±n maliyetini hesaplamak.

#### Ä°ÅŸleyiÅŸ
```python
def _get_operation_cost(ref_token: str, hyp_token: str, operation: str, 
                       repeated_fillers: Dict[int, bool] = None, hyp_idx: int = -1) -> float:
    if operation == "equal":
        return 0.0
    elif operation in ["insert", "delete"]:
        base_cost = 1.0
        
        # Stopword'ler iÃ§in dÃ¼ÅŸÃ¼k cost
        token = ref_token if operation == "delete" else hyp_token
        if _is_stop(token):
            base_cost = 0.4
        
        # Filler word handling
        if operation == "insert" and _is_filler(hyp_token):
            if repeated_fillers and hyp_idx in repeated_fillers and repeated_fillers[hyp_idx]:
                return max(0.1, base_cost - 0.3)  # Repeated filler bonus
            else:
                return base_cost
        
        return base_cost
    elif operation == "replace":
        # Punctuation substitution - forbid
        if _is_punctuation(ref_token) or _is_punctuation(hyp_token):
            return float('inf')
        
        # Filler substitution - forbid
        if _is_filler(hyp_token) and not _is_filler(ref_token):
            return float('inf')
        
        # Stopword substitution - higher cost
        return 1.2 if (_is_stop(ref_token) or _is_stop(hyp_token)) else 1.0
```

#### ğŸš¨ **Kritik Hata NoktasÄ±**
- **Problem**: `base_cost - 0.3` negatif olabilir
- **Ã–rnek**: `base_cost = 0.4`, sonuÃ§ = `0.1` (Ã§ok dÃ¼ÅŸÃ¼k cost)
- **SonuÃ§**: YanlÄ±ÅŸ filler classification

### 4. **Filler Word Tracking** (`_track_filler_repetitions`)

#### AmaÃ§
2 saniye iÃ§inde tekrarlanan filler word'leri tespit etmek.

#### Ä°ÅŸleyiÅŸ
```python
def _track_filler_repetitions(hyp_tokens: List[str], word_times: List[Dict[str, Any]]) -> Dict[int, bool]:
    repetition_window_ms = 2000  # 2 seconds
    filler_counts = {}  # filler_word -> list of (timestamp, index)
    repeated_fillers = {}
    
    for i, (token, timing) in enumerate(zip(hyp_tokens, word_times)):
        if _is_filler(token) and timing and 'start' in timing:
            start_ms = timing['start'] * 1000
            filler_word = _norm_token(token)
            
            if filler_word not in filler_counts:
                filler_counts[filler_word] = []
            
            # Add current occurrence
            filler_counts[filler_word].append((start_ms, i))
            
            # Remove old occurrences outside window
            filler_counts[filler_word] = [
                (ts, idx) for ts, idx in filler_counts[filler_word] 
                if start_ms - ts <= repetition_window_ms
            ]
            
            # Mark as repeated if appears 2+ times in window
            if len(filler_counts[filler_word]) >= 2:
                repeated_fillers[i] = True
            else:
                repeated_fillers[i] = False
    
    return repeated_fillers
```

#### ğŸš¨ **Kritik Hata NoktasÄ±**
- **Problem**: `word_times` yoksa tÃ¼m filler'lar normal cost alÄ±r
- **SonuÃ§**: Inconsistent filler handling

### 5. **Repetition Pattern Detection**

#### AmaÃ§
`Ã¶ÄŸret-Ã¶ÄŸretmen` gibi tekrarlama pattern'lerini tespit etmek.

#### Ä°ÅŸleyiÅŸ
```python
# Special case: "Ã¶ÄŸret-Ã¶ÄŸretmen" pattern
is_repetition_pattern = False
if ("-" in hyp_token and not hyp_token.startswith("-") and not hyp_token.endswith("-")):
    parts = hyp_token.split("-", 1)
    if len(parts) > 1:
        norm_hyp_after_dash = _norm_token(parts[1])
        norm_ref = _norm_token(ref_token)
        if norm_hyp_after_dash == norm_ref:
            is_repetition_pattern = True

if is_repetition_pattern:
    # Treat as repetition pattern
    alignment.append(("replace", ref_token, hyp_token, i-1, j-1))
```

#### ğŸš¨ **Kritik Hata NoktasÄ±**
- **Problem**: Her `-` iÃ§eren kelimeyi repetition olarak algÄ±layabilir
- **Ã–rnek**: `tÃ¼rk-amerikan` â†’ yanlÄ±ÅŸ repetition detection
- **SonuÃ§**: False positive repetition events

### 6. **Backtracking Logic**

#### AmaÃ§
DP tablosundan optimal alignment path'ini Ã§Ä±karmak.

#### Ä°ÅŸleyiÅŸ
```python
while i > 0 or j > 0:
    ref_token = ref_tokens[i-1] if i > 0 else ""
    hyp_token = hyp_tokens[j-1] if j > 0 else ""
    
    # Normalized equality check
    if i > 0 and j > 0 and _norm_token(ref_token) == _norm_token(hyp_token):
        alignment.append(("equal", ref_token, hyp_token, i-1, j-1))
        i -= 1
        j -= 1
    elif i > 0 and (j == 0 or dp[i-1][j] < dp[i][j-1]):
        # Delete - skip punctuation
        if _is_punctuation(ref_token):
            i -= 1  # Skip punctuation completely
        else:
            alignment.append(("delete", ref_token, "", i-1, -1))
            i -= 1
    # ... diÄŸer durumlar
```

#### ğŸš¨ **Kritik Hata NoktasÄ±**
- **Problem**: `i -= 1` ile `continue` eksik olabilir
- **SonuÃ§**: Infinite loop riski

### 7. **Post-Repair Logic** (`_post_repair_filler_substitutions`)

#### AmaÃ§
Problemli filler substitution'larÄ± MISSING+EXTRA pair'lerine Ã§evirmek.

#### Ä°ÅŸleyiÅŸ
```python
def _post_repair_filler_substitutions(alignment: List[Tuple[str, str, str, int, int]]) -> List[Tuple[str, str, str, int, int]]:
    repaired = []
    i = 0
    
    while i < len(alignment):
        op, ref_token, hyp_token, ref_idx, hyp_idx = alignment[i]
        
        if op == "replace" and _is_filler(hyp_token) and not _is_filler(ref_token):
            # Check if next alignment has high similarity
            next_similar = False
            if i + 1 < len(alignment):
                next_op, next_ref, next_hyp, _, _ = alignment[i + 1]
                if next_op in ["equal", "replace"] and next_ref and next_hyp:
                    # Calculate normalized Levenshtein distance
                    lev_dist = char_edit_stats(ref_token, next_hyp)[0]
                    max_len = max(len(ref_token), len(next_hyp))
                    lev_norm = lev_dist / max_len if max_len > 0 else 1.0
                    
                    if lev_norm <= 0.3:  # High similarity threshold
                        next_similar = True
            
            if next_similar:
                # Convert SUB to MISSING + EXTRA
                repaired.append(("delete", ref_token, "", ref_idx, -1))
                repaired.append(("insert", "", hyp_token, -1, hyp_idx))
            else:
                repaired.append((op, ref_token, hyp_token, ref_idx, hyp_idx))
        else:
            repaired.append((op, ref_token, hyp_token, ref_idx, hyp_idx))
        
        i += 1
    
    return repaired
```

#### ğŸš¨ **Kritik Hata NoktasÄ±**
- **Problem**: Magic number `0.3` threshold sabit
- **SonuÃ§**: FarklÄ± context'ler iÃ§in uygun olmayabilir

### 8. **Word Event Building** (`build_word_events`)

#### AmaÃ§
Alignment sonuÃ§larÄ±ndan WordEventDoc formatÄ±nda event'ler oluÅŸturmak.

#### Ä°ÅŸleyiÅŸ
```python
def build_word_events(alignment: List[Tuple[str, str, str, int, int]], word_times: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    word_events = []
    
    for op, ref_token, hyp_token, ref_idx, hyp_idx in alignment:
        # Skip punctuation tokens
        if _is_punctuation(ref_token) or _is_punctuation(hyp_token):
            if (ref_token and hyp_token and 
                _is_punctuation(ref_token) and _is_punctuation(hyp_token) and 
                ref_token == hyp_token):
                event_type = "correct"
                subtype = None
            else:
                continue  # Skip punctuation
        
        # Check for normalized equality
        elif _norm_token(ref_token) == _norm_token(hyp_token):
            event_type = "correct"
            subtype = "case_punct_only"
        elif op == "equal":
            event_type = "correct"
            subtype = None
        elif op == "delete":
            event_type = "missing"
            subtype = None
        elif op == "insert":
            event_type = "extra"
            subtype = None
        elif op == "replace":
            # Check for repetition pattern
            is_repetition_pattern = False
            if ("-" in hyp_token and not hyp_token.startswith("-") and not hyp_token.endswith("-")):
                parts = hyp_token.split("-", 1)
                if len(parts) > 1:
                    norm_hyp_after_dash = _norm_token(parts[1])
                    norm_ref = _norm_token(ref_token)
                    if norm_hyp_after_dash == norm_ref:
                        is_repetition_pattern = True
            
            if is_repetition_pattern:
                event_type = "repetition"
                subtype = "enhanced_pattern"
            else:
                event_type = "substitution"
                subtype = classify_replace(ref_token, hyp_token)
                subtype = normalize_sub_type(subtype)
        
        # Get timing data
        start_ms = None
        end_ms = None
        if hyp_idx >= 0 and hyp_idx < len(word_times):
            start_ms = word_times[hyp_idx].get("start", 0) * 1000
            end_ms = word_times[hyp_idx].get("end", 0) * 1000
        
        event_data = {
            "ref_token": ref_token if ref_token else None,
            "hyp_token": hyp_token if hyp_token else None,
            "start_ms": start_ms,
            "end_ms": end_ms,
            "type": event_type,
            "sub_type": subtype,
            "ref_idx": ref_idx,
            "hyp_idx": hyp_idx
        }
        
        word_events.append(event_data)
    
    return word_events
```

#### ğŸš¨ **Kritik Hata NoktasÄ±**
- **Problem**: Double normalization (hem DP'de hem event building'de)
- **SonuÃ§**: Inconsistent classification

## ğŸš¨ **Kritik Algoritma HatalarÄ±**

### 1. **Normalizasyon Ã‡ok Agresif**
- **Problem**: TÃ¼rkÃ§e karakterler Ã§ok agresif normalize ediliyor
- **Etki**: FarklÄ± anlamlÄ± kelimeler aynÄ± olarak algÄ±lanÄ±yor
- **Ã–rnek**: `Ä±ÅŸÄ±k` â†’ `isik`, `gÃ¶z` â†’ `goz`

### 2. **Filler Tracking Timing-Dependent**
- **Problem**: `word_times` yoksa filler logic Ã§alÄ±ÅŸmÄ±yor
- **Etki**: Inconsistent filler handling
- **SonuÃ§**: BazÄ± durumlarda filler'lar normal cost alÄ±yor

### 3. **Operation Costs Negative Olabilir**
- **Problem**: `base_cost - 0.3` negatif sonuÃ§ verebilir
- **Etki**: DP table corruption
- **SonuÃ§**: YanlÄ±ÅŸ alignment path'leri

### 4. **Repetition Pattern Over-Matching**
- **Problem**: Her `-` iÃ§eren kelimeyi repetition olarak algÄ±lÄ±yor
- **Etki**: False positive repetition detection
- **Ã–rnek**: `tÃ¼rk-amerikan` â†’ yanlÄ±ÅŸ repetition

### 5. **Backtracking Complexity**
- **Problem**: Complex backtracking logic
- **Etki**: Index errors ve infinite loop riski
- **SonuÃ§**: System crash

### 6. **Magic Numbers**
- **Problem**: Sabit threshold'lar (0.3, 2000ms, 0.4)
- **Etki**: Non-adaptive behavior
- **SonuÃ§**: FarklÄ± context'ler iÃ§in uygun olmayabilir

### 7. **Double Normalization**
- **Problem**: Hem DP'de hem event building'de normalizasyon
- **Etki**: Inconsistent results
- **SonuÃ§**: AynÄ± durum farklÄ± ÅŸekilde sÄ±nÄ±flandÄ±rÄ±labilir

## ğŸ“Š **Performans Analizi**

### Time Complexity
- **Levenshtein DP**: O(mÃ—n) where m=ref_tokens, n=hyp_tokens
- **Filler tracking**: O(n) where n=hyp_tokens
- **Post-repair**: O(k) where k=alignment_length
- **Overall**: O(mÃ—n) - optimal for alignment problem

### Space Complexity
- **DP table**: O(mÃ—n)
- **Filler tracking**: O(n)
- **Overall**: O(mÃ—n)

### Memory Usage
- **DP table**: 8 bytes per cell (float64)
- **Example**: 1000Ã—1000 tokens = ~8MB memory

## ğŸ¯ **Ã–nerilen Ä°yileÅŸtirmeler**

### 1. **Normalizasyon Ä°yileÅŸtirmesi**
```python
def _norm_token_conservative(tok: str) -> str:
    """Daha conservative normalizasyon"""
    # Sadece case normalization
    t = tok.lower()
    
    # Sadece temel noktalama temizleme
    t = re.sub(r'[.,!?;:"""]+$', '', t)
    t = re.sub(r'^[.,!?;:"""]+', '', t)
    
    return t
```

### 2. **Filler Tracking Ä°yileÅŸtirmesi**
```python
def _track_filler_repetitions_robust(hyp_tokens: List[str], word_times: List[Dict[str, Any]] = None) -> Dict[int, bool]:
    """Timing-independent filler tracking"""
    if not word_times:
        # Fallback: simple repetition detection
        return _simple_filler_repetition_detection(hyp_tokens)
    else:
        return _timing_based_filler_repetition_detection(hyp_tokens, word_times)
```

### 3. **Operation Cost Ä°yileÅŸtirmesi**
```python
def _get_operation_cost_safe(ref_token: str, hyp_token: str, operation: str, 
                           repeated_fillers: Dict[int, bool] = None, hyp_idx: int = -1) -> float:
    """Safe operation cost calculation"""
    if operation == "insert" and _is_filler(hyp_token):
        if repeated_fillers and hyp_idx in repeated_fillers and repeated_fillers[hyp_idx]:
            # Ensure minimum cost
            return max(0.2, base_cost - 0.2)
        else:
            return base_cost
```

### 4. **Repetition Pattern Ä°yileÅŸtirmesi**
```python
def _is_valid_repetition_pattern(ref_token: str, hyp_token: str) -> bool:
    """Daha strict repetition pattern detection"""
    if "-" not in hyp_token:
        return False
    
    # Check if it's a valid Turkish repetition pattern
    parts = hyp_token.split("-", 1)
    if len(parts) != 2:
        return False
    
    # Additional validation
    if len(parts[0]) < 3 or len(parts[1]) < 3:  # Minimum length
        return False
    
    # Check if first part is prefix of second part
    if not parts[1].startswith(parts[0]):
        return False
    
    return _norm_token(parts[1]) == _norm_token(ref_token)
```

### 5. **Backtracking Ä°yileÅŸtirmesi**
```python
def _safe_backtrack(dp: List[List[float]], ref_tokens: List[str], hyp_tokens: List[str]) -> List[Tuple[str, str, str, int, int]]:
    """Safe backtracking with proper error handling"""
    alignment = []
    i, j = len(ref_tokens), len(hyp_tokens)
    
    while i > 0 or j > 0:
        try:
            # Safe index access
            ref_token = ref_tokens[i-1] if i > 0 else ""
            hyp_token = hyp_tokens[j-1] if j > 0 else ""
            
            # ... backtracking logic with proper bounds checking
            
        except (IndexError, ValueError) as e:
            logger.error(f"Backtracking error at position ({i}, {j}): {e}")
            break
    
    return alignment
```

## ğŸ” **Test SenaryolarÄ±**

### 1. **Normalizasyon Testi**
```python
def test_normalization():
    test_cases = [
        ("Ä±ÅŸÄ±k", "isik", True),  # Should be different
        ("gÃ¶z", "goz", True),    # Should be different  
        ("Ã§ok", "cok", False),   # Should be same
        ("Ã¶ÄŸretmen", "ogretmen", False),  # Should be same
    ]
    
    for ref, hyp, should_differ in test_cases:
        result = _norm_token(ref) == _norm_token(hyp)
        assert result != should_differ, f"Normalization error: {ref} vs {hyp}"
```

### 2. **Filler Tracking Testi**
```python
def test_filler_tracking():
    hyp_tokens = ["Ã§ok", "gÃ¼zel", "Ã§ok", "bir", "gÃ¼n"]
    word_times = [
        {"start": 0.0, "end": 0.5},
        {"start": 0.6, "end": 1.0}, 
        {"start": 1.1, "end": 1.6},  # 0.1s gap - should be repeated
        {"start": 1.7, "end": 2.0},
        {"start": 2.1, "end": 2.5}
    ]
    
    result = _track_filler_repetitions(hyp_tokens, word_times)
    assert result[2] == True, "Second 'Ã§ok' should be marked as repeated"
```

### 3. **Repetition Pattern Testi**
```python
def test_repetition_patterns():
    test_cases = [
        ("Ã¶ÄŸretmen", "Ã¶ÄŸret-Ã¶ÄŸretmen", True),   # Valid repetition
        ("tÃ¼rk", "tÃ¼rk-amerikan", False),       # Invalid repetition
        ("kitap", "kitap-kitap", True),         # Valid repetition
        ("okul", "okul-Ã¶ÄŸretmen", False),       # Invalid repetition
    ]
    
    for ref, hyp, should_be_repetition in test_cases:
        result = _is_valid_repetition_pattern(ref, hyp)
        assert result == should_be_repetition, f"Repetition error: {ref} vs {hyp}"
```

## ğŸ“ˆ **Metrikler ve DeÄŸerlendirme**

### 1. **Accuracy Metrics**
- **Alignment Accuracy**: DoÄŸru alignment oranÄ±
- **Error Classification Accuracy**: Hata sÄ±nÄ±flandÄ±rma doÄŸruluÄŸu
- **False Positive Rate**: YanlÄ±ÅŸ pozitif oranÄ±
- **False Negative Rate**: YanlÄ±ÅŸ negatif oranÄ±

### 2. **Performance Metrics**
- **Processing Time**: Alignment iÅŸlem sÃ¼resi
- **Memory Usage**: Bellek kullanÄ±mÄ±
- **Throughput**: Dakikada iÅŸlenen kelime sayÄ±sÄ±

### 3. **Quality Metrics**
- **WER (Word Error Rate)**: Kelime hata oranÄ±
- **CER (Character Error Rate)**: Karakter hata oranÄ±
- **Alignment Quality Score**: Alignment kalite skoru

## ğŸ¯ **SonuÃ§ ve Ã–neriler**

Alignment algoritmasÄ± genel olarak iyi tasarlanmÄ±ÅŸ ancak **kritik hata noktalarÄ±** bulunmaktadÄ±r. En Ã¶nemli sorunlar:

1. **Normalizasyon Ã§ok agresif** â†’ False positive matches
2. **Filler tracking timing-dependent** â†’ Inconsistent behavior
3. **Operation costs negative olabilir** â†’ DP table corruption
4. **Repetition pattern over-matching** â†’ False repetition detection

**Ã–ncelikli iyileÅŸtirmeler:**
1. Conservative normalizasyon implementasyonu
2. Robust filler tracking (timing-independent)
3. Safe operation cost calculation
4. Strict repetition pattern validation
5. Comprehensive test suite

Bu iyileÅŸtirmeler yapÄ±ldÄ±ÄŸÄ±nda, algoritmanÄ±n accuracy'si ve reliability'si Ã¶nemli Ã¶lÃ§Ã¼de artacaktÄ±r.

---

**Rapor Tarihi**: 2024-12-19  
**Versiyon**: 1.0  
**HazÄ±rlayan**: AI Code Assistant  
**Dosya**: `backend/app/services/alignment.py`
